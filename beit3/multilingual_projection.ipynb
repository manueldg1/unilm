{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e434d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/home/u036743/emotion_recognition/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "import math\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models import create_model\n",
    "from timm.models.layers import trunc_normal_ as __call_trunc_normal__\n",
    "from timm.models.registry import register_model\n",
    "from torchscale.model.BEiT3 import BEiT3\n",
    "from torchscale.architecture.config import EncoderConfig\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote xlmr.txt with 250002 tokens × 1024 dimensions\n"
     ]
    }
   ],
   "source": [
    "# 1) load the pretrained tokenizer and model\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "# 2) get ordered vocab list\n",
    "#    (tokenizer.get_vocab() returns a dict token→id)\n",
    "vocab = sorted(tokenizer.get_vocab().items(), key=lambda x: x[1])\n",
    "tokens, ids = zip(*vocab)\n",
    "\n",
    "# 3) extract embedding matrix (|V|×D)\n",
    "embs = model.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# 4) write Word2Vec text file\n",
    "D = embs.shape[1]\n",
    "with open(\"xlmr.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(f\"{len(tokens)} {D}\\n\")\n",
    "    for tok, vec in zip(tokens, embs):\n",
    "        vec_str = \" \".join(f\"{x:.6f}\" for x in vec)\n",
    "        fout.write(f\"{tok} {vec_str}\\n\")\n",
    "\n",
    "print(\"Wrote xlmr.txt with\", len(tokens), \"tokens ×\", D, \"dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6decfb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Loading model checkpoint from /cfs/home/u036743/emotion_recognition/beit3_large_patch16_224.pth\n",
      "Model loaded\n",
      "Wrote beit3.txt with 64010 tokens × 1024 dimensions\n"
     ]
    }
   ],
   "source": [
    "# dump_beit3.py\n",
    "\"\"\"\n",
    "Export BEiT-3 text embeddings to Word2Vec-style text.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration constants\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Directory of the saved the BEIT-3 tokenizer\n",
    "TOKENIZER_DIR = \"/cfs/home/u036743/emotion_recognition/beit3.spm\"\n",
    "# 2) Name or path of the BEIT-3 model checkpoint\n",
    "MODEL_NAME_OR_PATH = \"/cfs/home/u036743/emotion_recognition/beit3_large_patch16_224.pth\"\n",
    "# 5) Output file\n",
    "OUTPUT_TXT = \"beit3.txt\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility / config builders\n",
    "# ---------------------------------------------------------------------\n",
    "def trunc_normal_(tensor, mean: float = 0.0, std: float = 1.0) -> None:\n",
    "    __call_trunc_normal__(tensor, mean=mean, std=std, a=-std, b=std)\n",
    "\n",
    "\n",
    "def _get_base_config(img_size: int = 224,\n",
    "                     patch_size: int = 16,\n",
    "                     drop_path_rate: float = 0,\n",
    "                     checkpoint_activations=None,\n",
    "                     mlp_ratio: int = 4,\n",
    "                     vocab_size: int = 64010,\n",
    "                     **kwargs) -> EncoderConfig:\n",
    "    return EncoderConfig(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        vocab_size=vocab_size,\n",
    "        multiway=True,\n",
    "        layernorm_embedding=False,\n",
    "        normalize_output=True,\n",
    "        no_output_layer=True,\n",
    "        drop_path_rate=drop_path_rate,\n",
    "        encoder_embed_dim=768,\n",
    "        encoder_attention_heads=12,\n",
    "        encoder_ffn_embed_dim=int(768 * mlp_ratio),\n",
    "        encoder_layers=12,\n",
    "        checkpoint_activations=checkpoint_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_large_config(img_size: int = 224,\n",
    "                      patch_size: int = 16,\n",
    "                      drop_path_rate: float = 0,\n",
    "                      checkpoint_activations=None,\n",
    "                      mlp_ratio: int = 4,\n",
    "                      vocab_size: int = 64010,\n",
    "                      **kwargs) -> EncoderConfig:\n",
    "    return EncoderConfig(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        vocab_size=vocab_size,\n",
    "        multiway=True,\n",
    "        layernorm_embedding=False,\n",
    "        normalize_output=True,\n",
    "        no_output_layer=True,\n",
    "        drop_path_rate=drop_path_rate,\n",
    "        encoder_embed_dim=1024,\n",
    "        encoder_attention_heads=16,\n",
    "        encoder_ffn_embed_dim=int(1024 * mlp_ratio),\n",
    "        encoder_layers=24,\n",
    "        checkpoint_activations=checkpoint_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Model wrapper and registration\n",
    "# ---------------------------------------------------------------------\n",
    "class BEiT3Wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.beit3 = BEiT3(args)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def fix_init_weight(self) -> None:\n",
    "\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def get_num_layers(self) -> int:\n",
    "        return self.beit3.encoder.num_layers\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\n",
    "            'pos_embed', 'cls_token', 'beit3.encoder.embed_positions.A.weight',\n",
    "            'beit3.vision_embed.cls_token', 'logit_scale'\n",
    "        }\n",
    "\n",
    "    def _init_weights(self, m) -> None:\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.beit3.text_embed\n",
    "\n",
    "\n",
    "@register_model\n",
    "def beit3_large_patch16_224(pretrained: bool = False, **kwargs) -> BEiT3Wrapper:\n",
    "    args = _get_large_config(**kwargs)\n",
    "    args.normalize_output = False\n",
    "    model = BEiT3Wrapper(args, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Loading helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def load_checkpoint(path: str):\n",
    "    \"\"\"Load a checkpoint and return its state dict.\"\"\"\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    if \"model\" in ckpt:\n",
    "        return ckpt[\"model\"]\n",
    "    if \"state_dict\" in ckpt:\n",
    "        return ckpt[\"state_dict\"]\n",
    "    return ckpt\n",
    "\n",
    "\n",
    "def build_registered_model() -> nn.Module:\n",
    "    model = create_model(\n",
    "        \"beit3_large_patch16_224\",\n",
    "        pretrained=False,\n",
    "        drop_path_rate=0.1,\n",
    "        vocab_size=64010,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Tokenizer / vocab and export helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def load_tokenizer(tokenizer_dir: str) -> XLMRobertaTokenizer:\n",
    "    \"\"\"Instantiate XLM-R tokenizer from directory.\"\"\"\n",
    "    return XLMRobertaTokenizer(tokenizer_dir)\n",
    "\n",
    "\n",
    "def extract_sorted_vocab(\n",
    "        tokenizer: XLMRobertaTokenizer) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Return tokens and ids sorted by id.\"\"\"\n",
    "    vocab_items = sorted(tokenizer.get_vocab().items(), key=lambda x: x[1])\n",
    "    tokens, ids = zip(*vocab_items)\n",
    "    return list(tokens), list(ids)\n",
    "\n",
    "\n",
    "def export_embeddings_txt(tokens: List[str], embeddings: torch.Tensor,\n",
    "                          out_path: str) -> None:\n",
    "    \"\"\"Write Word2Vec-style text file.\"\"\"\n",
    "    embs = embeddings.detach().cpu().numpy()\n",
    "    V, D = embs.shape\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        fout.write(f\"{V} {D}\\n\")\n",
    "        for tok, vec in zip(tokens, embs):\n",
    "            vec_str = \" \".join(f\"{x:.6f}\" for x in vec)\n",
    "            fout.write(f\"{tok} {vec_str}\\n\")\n",
    "    print(f\"Wrote {out_path} with {V} tokens × {D} dimensions\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer and model\n",
    "    print(\"Loading tokenizer\")\n",
    "\n",
    "    # Instantiate tokenizer\n",
    "    tokenizer = load_tokenizer(TOKENIZER_DIR)\n",
    "\n",
    "    # Create and load the timm-registered model\n",
    "    model = build_registered_model()\n",
    "    print(\"Loading model checkpoint from\", MODEL_NAME_OR_PATH)\n",
    "    sd = load_checkpoint(MODEL_NAME_OR_PATH)\n",
    "    model.load_state_dict(sd, strict=False)\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    # Extract vocab sorted by id\n",
    "    tokens, _ = extract_sorted_vocab(tokenizer)\n",
    "\n",
    "    embs_weight = model.get_input_embeddings().weight\n",
    "\n",
    "    # Export in Word2Vec text format\n",
    "    export_embeddings_txt(tokens, embs_weight, OUTPUT_TXT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146d1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEiT-3 vocab size: 64002\n",
      "XLM-R vocab size:  250002\n",
      "Common tokens:     22076\n",
      "Wrote seed dictionary to /cfs/home/u036743/emotion_recognition/out_seed.txt\n"
     ]
    }
   ],
   "source": [
    "def load_vocab(path):\n",
    "    \"\"\"\n",
    "    Load the vocab (first column) from a word2vec-format file.\n",
    "    Assumes the first line is \"V D\" header; skips it.\n",
    "    \"\"\"\n",
    "    vocab = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        header = f.readline()  # e.g. \"64010 1024\"\n",
    "        for line in f:\n",
    "            tok = line.split(' ', 1)[0]\n",
    "            vocab.append(tok)\n",
    "    return vocab\n",
    "\n",
    "def main(beit3_path, xlmr_path, out_seed_path):\n",
    "    beit3_vocab = load_vocab(beit3_path)\n",
    "    xlmr_vocab  = load_vocab(xlmr_path)\n",
    "\n",
    "    set_beit = set(beit3_vocab)\n",
    "    set_xlmr = set(xlmr_vocab)\n",
    "\n",
    "    # Intersection: common tokens\n",
    "    common = sorted(set_beit & set_xlmr)\n",
    "\n",
    "    print(f\"BEiT-3 vocab size: {len(beit3_vocab)}\")\n",
    "    print(f\"XLM-R vocab size:  {len(xlmr_vocab)}\")\n",
    "    print(f\"Common tokens:     {len(common)}\")\n",
    "\n",
    "    # Write seed dictionary\n",
    "    with open(out_seed_path, 'w', encoding='utf-8') as fout:\n",
    "        for tok in common:\n",
    "            fout.write(f\"{tok} {tok}\\n\")\n",
    "\n",
    "    print(f\"Wrote seed dictionary to {out_seed_path}\")\n",
    "    \n",
    "beit3_path, xlmr_path, out_seed_path = \"/cfs/home/u036743/emotion_recognition/beit3.txt\", \"/cfs/home/u036743/emotion_recognition/xlmr.txt\", \"/cfs/home/u036743/emotion_recognition/out_seed.txt\"\n",
    "main(beit3_path, xlmr_path, out_seed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b3a0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64003 /cfs/home/u036743/emotion_recognition/beit3_clean.txt\n",
      "250003 /cfs/home/u036743/emotion_recognition/xlmr_clean.txt\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Operations that you can perform here or on terminal\n",
    "# Count lines\n",
    "!wc -l /cfs/home/u036743/emotion_recognition/beit3_clean.txt\n",
    "!wc -l /cfs/home/u036743/emotion_recognition/xlmr_clean.txt\n",
    "\n",
    "# Check embedding dimension\n",
    "!head -1 /cfs/home/u036743/emotion_recognition/beit3_clean.txt | awk '{print NF-1}'\n",
    "\n",
    "# Add header lines (prepend the number of tokens + dimension)\n",
    "!sed -i '1i 64002 1024' /cfs/home/u036743/emotion_recognition/beit3_clean.txt\n",
    "!sed -i '1i 250002 1024' /cfs/home/u036743/emotion_recognition/xlmr_clean.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84afcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mapped XLM-R vectors from /cfs/home/u036743/emotion_recognition/xlmr_mapped.txt\n",
      "Loading XLM-RoBERTa-large tokenizer from xlm-roberta-large\n",
      "Building new embedding matrix: 250002 tokens × 1024 dims\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3712322/3964696591.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  new_emb[idx] = torch.from_numpy(mapped_kv[token])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → 0 tokens were not found in the mapped file; randomly initialized them\n",
      "Saving new embeddings to xlmr_in_beit3_space.pt\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ─── Config ────────────────────────────────────────────────────────────────────\n",
    "XLMR_MAPPED_TXT = \"/cfs/home/u036743/emotion_recognition/xlmr_mapped.txt\"\n",
    "XLMR_TOKENIZER_DIR = \"xlm-roberta-large\"\n",
    "OUT_FILE = \"xlmr_in_beit3_space.pt\"\n",
    "\n",
    "# ─── 1) load mapped vectors ────────────────────────────────────────────────────\n",
    "print(\"Loading mapped XLM-R vectors from\", XLMR_MAPPED_TXT)\n",
    "mapped_kv = KeyedVectors.load_word2vec_format(XLMR_MAPPED_TXT, binary=False)\n",
    "\n",
    "# ─── 2) load XLM-RoBERTa tokenizer ───────────────────────────────────────────────\n",
    "print(\"Loading XLM-RoBERTa-large tokenizer from\", XLMR_TOKENIZER_DIR)\n",
    "tok = XLMRobertaTokenizer.from_pretrained(XLMR_TOKENIZER_DIR)\n",
    "\n",
    "vocab = tok.get_vocab()  # { token_str: token_id }\n",
    "vocab_size = len(vocab)\n",
    "D = mapped_kv.vector_size  # should be 1024\n",
    "\n",
    "assert D == 1024, f\"Expected mapped vectors to be 1024-dim, got {D}\"\n",
    "\n",
    "# ─── 3) allocate + fill ───────────────────────────────────────────────────────\n",
    "print(f\"Building new embedding matrix: {vocab_size} tokens × {D} dims\")\n",
    "new_emb = torch.zeros(vocab_size, D, dtype=torch.float32)\n",
    "\n",
    "missing = 0\n",
    "for token, idx in vocab.items():\n",
    "    if token in mapped_kv:\n",
    "        new_emb[idx] = torch.from_numpy(mapped_kv[token])\n",
    "    else:\n",
    "        # random init for any stray tokens\n",
    "        new_emb[idx].normal_(0, 0.02)\n",
    "        missing += 1\n",
    "\n",
    "print(\n",
    "    f\"  → {missing} tokens were not found in the mapped file; randomly initialized them\"\n",
    ")\n",
    "\n",
    "# ─── 4) save ───────────────────────────────────────────────────────────────────\n",
    "print(\"Saving new embeddings to\", OUT_FILE)\n",
    "torch.save(new_emb, OUT_FILE)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d89d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Backbone loaded (excluding text_embed).\n",
      "✓ Injected new embeddings.\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n",
    "XLRM_TOKENIZER_VOCAB_SIZE = 250002\n",
    "XLMR_EMB_PATH = \"xlmr_in_beit3_space.pt\"\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n",
    "\n",
    "\n",
    "def _get_large_config(img_size=224,\n",
    "                      patch_size=16,\n",
    "                      drop_path_rate=0,\n",
    "                      checkpoint_activations=None,\n",
    "                      mlp_ratio=4,\n",
    "                      vocab_size=XLRM_TOKENIZER_VOCAB_SIZE,\n",
    "                      **kwargs):\n",
    "    return EncoderConfig(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        vocab_size=vocab_size,\n",
    "        multiway=True,\n",
    "        layernorm_embedding=False,\n",
    "        normalize_output=True,\n",
    "        no_output_layer=True,\n",
    "        drop_path_rate=drop_path_rate,\n",
    "        encoder_embed_dim=1024,\n",
    "        encoder_attention_heads=16,\n",
    "        encoder_ffn_embed_dim=int(1024 * mlp_ratio),\n",
    "        encoder_layers=24,\n",
    "        checkpoint_activations=checkpoint_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "class BEiT3Wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.beit3 = BEiT3(args)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return self.beit3.encoder.num_layers\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\n",
    "            'pos_embed', 'cls_token', 'beit3.encoder.embed_positions.A.weight',\n",
    "            'beit3.vision_embed.cls_token', 'logit_scale'\n",
    "        }\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def beit3_large_patch16_224(pretrained=False, **kwargs):\n",
    "    args = _get_large_config(**kwargs)\n",
    "    args.normalize_output = False\n",
    "    model = BEiT3Wrapper(args, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint_dict(path: str) -> dict:\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    if \"model\" in ckpt:\n",
    "        return ckpt[\"model\"]\n",
    "    if \"state_dict\" in ckpt:\n",
    "        return ckpt[\"state_dict\"]\n",
    "    return ckpt\n",
    "\n",
    "\n",
    "def inject_new_embeddings(model: nn.Module, new_emb: torch.Tensor):\n",
    "    # Access the text embedding layer directly from the wrapper\n",
    "    layer = model.beit3.text_embed\n",
    "    assert layer.weight.shape == new_emb.shape, (\n",
    "        f\"Expected {layer.weight.shape}, got {new_emb.shape}\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        layer.weight.copy_(new_emb)\n",
    "\n",
    "\n",
    "def load_beit3_model(model_name,\n",
    "                     tokenizer_model_name=\"xlmr-roberta-large\",\n",
    "                     xlmr_embeddings_path=XLMR_EMB_PATH,\n",
    "                     checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    Load the BEiT3 model with the specified model name and checkpoint path.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(tokenizer_model_name)\n",
    "\n",
    "    # Create the model\n",
    "    model = create_model(\n",
    "        model_name,\n",
    "        drop_path_rate=0.1,\n",
    "    )\n",
    "\n",
    "    # Load the checkpoint if provided\n",
    "    if checkpoint_path:\n",
    "        sd = load_checkpoint_dict(checkpoint_path)\n",
    "\n",
    "        # Remove the text_embed.weight from the state dict\n",
    "        # Because we will inject the new embeddings\n",
    "        sd.pop(\"beit3.text_embed.weight\", None)\n",
    "        \n",
    "        model.load_state_dict(sd, strict=False)\n",
    "        print(\"✓ Backbone loaded (excluding text_embed).\")\n",
    "\n",
    "        # Inject mapped embeddings\n",
    "        # Load new XLM-R→BEiT-3 embeddings\n",
    "        new_emb = torch.load(xlmr_embeddings_path)\n",
    "\n",
    "        # Confirm vocab size matches XLM-R tokenizer\n",
    "        assert len(tokenizer) == new_emb.shape[0], (\n",
    "            f\"Tokenizer vocab ({len(tokenizer)}) \"\n",
    "            f\"!= embeddings ({new_emb.shape[0]})\")\n",
    "\n",
    "        # Inject new embeddings\n",
    "        inject_new_embeddings(model, new_emb)\n",
    "        print(\"✓ Injected new embeddings.\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    model_name = \"beit3_large_patch16_224\"\n",
    "    tokenizer_model_name = \"xlm-roberta-large\"\n",
    "    xlmr_embeddings_path = XLMR_EMB_PATH\n",
    "    checkpoint_path = \"/cfs/home/u036743/emotion_recognition/beit3_large_patch16_224.pth\"\n",
    "\n",
    "    model, tokenizer = load_beit3_model(model_name, tokenizer_model_name,\n",
    "                                        xlmr_embeddings_path, checkpoint_path)\n",
    "\n",
    "    print(\"Model and tokenizer loaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "emotion_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
